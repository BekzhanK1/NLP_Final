{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "f6b8df4a",
            "metadata": {},
            "source": [
                "# Task 3: Machine Translation\n",
                "\n",
                "## Goal\n",
                "Explore how multilingual and specialist translation models perform across datasets and genres, and evaluate their quality using standard metrics.\n",
                "\n",
                "**Language Pair:** English ↔ Russian\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f19d1377",
            "metadata": {},
            "source": [
                "## 1. Setup and Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b7011afc",
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install transformers sacrebleu datasets sentencepiece accelerate deep-translator torchaudio huggingface_hub -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7ac51684",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, NllbTokenizer\n",
                "from datasets import load_dataset\n",
                "import sacrebleu\n",
                "from deep_translator import GoogleTranslator\n",
                "import pandas as pd\n",
                "from tqdm.auto import tqdm\n",
                "import os\n",
                "\n",
                "# Optional: Hugging Face authentication for gated datasets\n",
                "try:\n",
                "    from huggingface_hub import login\n",
                "    HF_HUB_AVAILABLE = True\n",
                "except ImportError:\n",
                "    HF_HUB_AVAILABLE = False\n",
                "    print(\"Note: huggingface_hub not installed. Install with: pip install huggingface_hub\")\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "49a020ba",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hugging Face authentication (optional - only needed for gated datasets like FLORES)\n",
                "hf_token = None\n",
                "if HF_HUB_AVAILABLE:\n",
                "    hf_token = os.environ.get('HUGGING_FACE_HUB_TOKEN') or os.environ.get('HF_TOKEN')\n",
                "    \n",
                "    if hf_token:\n",
                "        try:\n",
                "            login(token=hf_token)\n",
                "            print(\"✓ Authenticated with Hugging Face\")\n",
                "        except Exception as e:\n",
                "            print(f\"Note: Could not authenticate with Hugging Face: {e}\")\n",
                "    else:\n",
                "        print(\"Note: No Hugging Face token found. FLORES dataset may require authentication.\")\n",
                "        print(\"To access FLORES:\")\n",
                "        print(\"  1. Get a token from https://huggingface.co/settings/tokens\")\n",
                "        print(\"  2. Request access at https://huggingface.co/datasets/openlanguagedata/flores_plus\")\n",
                "        print(\"  3. Set environment variable: export HUGGING_FACE_HUB_TOKEN='your_token'\")\n",
                "else:\n",
                "    print(\"Note: huggingface_hub not available. FLORES dataset may require authentication.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "65e22309",
            "metadata": {},
            "source": [
                "## 2. Baseline Model: NLLB-200\n",
                "\n",
                "**Model:** [`facebook/nllb-200-distilled-600M`](https://huggingface.co/facebook/nllb-200-distilled-600M)\n",
                "\n",
                "A multilingual foundation model supporting 200+ languages."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6eaa12ff",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load NLLB Model and Tokenizer\n",
                "model_name_nllb = \"facebook/nllb-200-distilled-600M\"\n",
                "tokenizer_nllb = NllbTokenizer.from_pretrained(model_name_nllb)\n",
                "model_nllb = AutoModelForSeq2SeqLM.from_pretrained(model_name_nllb).to(device)\n",
                "print(f\"✓ Loaded {model_name_nllb}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bce45536",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Helper function for NLLB translation\n",
                "def translate_nllb(texts, src_lang, tgt_lang, model, tokenizer, batch_size=8):\n",
                "    # NLLB codes: English -> eng_Latn, Russian -> rus_Cyrl\n",
                "    lang_codes = {\n",
                "        'en': 'eng_Latn',\n",
                "        'ru': 'rus_Cyrl'\n",
                "    }\n",
                "    \n",
                "    tokenizer.src_lang = lang_codes[src_lang]\n",
                "    tgt_lang_code = lang_codes[tgt_lang]\n",
                "    \n",
                "    # Get the target language token ID\n",
                "    try:\n",
                "        forced_bos_token_id = tokenizer.convert_tokens_to_ids(tgt_lang_code)\n",
                "    except:\n",
                "        vocab = tokenizer.get_vocab()\n",
                "        if tgt_lang_code in vocab:\n",
                "            forced_bos_token_id = vocab[tgt_lang_code]\n",
                "        else:\n",
                "            if hasattr(tokenizer, 'lang_code_to_id'):\n",
                "                forced_bos_token_id = tokenizer.lang_code_to_id[tgt_lang_code]\n",
                "            else:\n",
                "                raise ValueError(f\"Could not find token ID for language code: {tgt_lang_code}\")\n",
                "    \n",
                "    translations = []\n",
                "    for i in tqdm(range(0, len(texts), batch_size), desc=f\"Translating {src_lang}->{tgt_lang}\"):\n",
                "        batch = texts[i:i+batch_size]\n",
                "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
                "        with torch.no_grad():\n",
                "            generated_tokens = model.generate(\n",
                "                **inputs,\n",
                "                forced_bos_token_id=forced_bos_token_id,\n",
                "                max_length=128\n",
                "            )\n",
                "        decoded = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
                "        translations.extend(decoded)\n",
                "    return translations"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3b6481d3",
            "metadata": {},
            "source": [
                "## 3. Second Model: Helsinki-NLP OPUS-MT\n",
                "\n",
                "**Models:** \n",
                "- [`Helsinki-NLP/opus-mt-en-ru`](https://huggingface.co/Helsinki-NLP/opus-mt-en-ru)\n",
                "- [`Helsinki-NLP/opus-mt-ru-en`](https://huggingface.co/Helsinki-NLP/opus-mt-ru-en)\n",
                "\n",
                "Specialist bilingual models optimized for English-Russian translation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "26eddc85",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Helsinki Models\n",
                "model_name_en_ru = \"Helsinki-NLP/opus-mt-en-ru\"\n",
                "tokenizer_en_ru = AutoTokenizer.from_pretrained(model_name_en_ru)\n",
                "model_en_ru = AutoModelForSeq2SeqLM.from_pretrained(model_name_en_ru).to(device)\n",
                "print(f\"✓ Loaded {model_name_en_ru}\")\n",
                "\n",
                "model_name_ru_en = \"Helsinki-NLP/opus-mt-ru-en\"\n",
                "tokenizer_ru_en = AutoTokenizer.from_pretrained(model_name_ru_en)\n",
                "model_ru_en = AutoModelForSeq2SeqLM.from_pretrained(model_name_ru_en).to(device)\n",
                "print(f\"✓ Loaded {model_name_ru_en}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c94ee155",
            "metadata": {},
            "outputs": [],
            "source": [
                "def translate_helsinki(texts, src_lang, tgt_lang, batch_size=32):\n",
                "    if src_lang == 'en' and tgt_lang == 'ru':\n",
                "        model = model_en_ru\n",
                "        tokenizer = tokenizer_en_ru\n",
                "    elif src_lang == 'ru' and tgt_lang == 'en':\n",
                "        model = model_ru_en\n",
                "        tokenizer = tokenizer_ru_en\n",
                "    else:\n",
                "        raise ValueError(\"Unsupported direction for Helsinki models loaded\")\n",
                "    \n",
                "    translations = []\n",
                "    for i in tqdm(range(0, len(texts), batch_size), desc=f\"Translating {src_lang}->{tgt_lang} (Helsinki)\"):\n",
                "        batch = texts[i:i+batch_size]\n",
                "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
                "        with torch.no_grad():\n",
                "            generated_tokens = model.generate(**inputs, max_length=128)\n",
                "        decoded = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
                "        translations.extend(decoded)\n",
                "    return translations"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "edeb6842",
            "metadata": {},
            "source": [
                "## 4. Custom Test Set\n",
                "\n",
                "**50 parallel sentences** (25 news + 25 fiction) for genre-specific evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c81234d7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define Custom Dataset\n",
                "custom_dataset = [\n",
                "    # News / Journalism (25 sentences)\n",
                "    {\"en\": \"The summit ended with a joint declaration on climate change.\", \"ru\": \"Саммит завершился принятием совместной декларации об изменении климата.\", \"genre\": \"news\"},\n",
                "    {\"en\": \"Local authorities have announced new measures to combat traffic congestion.\", \"ru\": \"Местные власти объявили о новых мерах по борьбе с пробками.\", \"genre\": \"news\"},\n",
                "    {\"en\": \"Scientists discovered a new species of orchid in the rainforest.\", \"ru\": \"Ученые обнаружили новый вид орхидей в тропическом лесу.\", \"genre\": \"news\"},\n",
                "    {\"en\": \"The stock market reacted positively to the latest economic reports.\", \"ru\": \"Фондовый рынок положительно отреагировал на последние экономические отчеты.\", \"genre\": \"news\"},\n",
                "    {\"en\": \"Education reform is a top priority for the new government.\", \"ru\": \"Реформа образования является главным приоритетом нового правительства.\", \"genre\": \"news\"},\n",
                "    {\"en\": \"The international conference will address global security challenges.\", \"ru\": \"Международная конференция рассмотрит глобальные вызовы безопасности.\", \"genre\": \"news\"},\n",
                "    {\"en\": \"Healthcare workers demand better working conditions and higher wages.\", \"ru\": \"Медицинские работники требуют улучшения условий труда и повышения зарплат.\", \"genre\": \"news\"},\n",
                "    {\"en\": \"The new policy aims to reduce carbon emissions by 40% by 2030.\", \"ru\": \"Новая политика направлена на сокращение выбросов углерода на 40% к 2030 году.\", \"genre\": \"news\"},\n",
                "    {\"en\": \"Tech companies are investing billions in artificial intelligence research.\", \"ru\": \"Технологические компании инвестируют миллиарды в исследования искусственного интеллекта.\", \"genre\": \"news\"},\n",
                "    {\"en\": \"The peace negotiations have reached a critical stage.\", \"ru\": \"Мирные переговоры достигли критической стадии.\", \"genre\": \"news\"},\n",
                "    {\"en\": \"Unemployment rates have dropped to their lowest level in a decade.\", \"ru\": \"Уровень безработицы упал до самого низкого уровня за десятилетие.\", \"genre\": \"news\"},\n",
                "    {\"en\": \"The archaeological discovery sheds new light on ancient civilizations.\", \"ru\": \"Археологическое открытие проливает новый свет на древние цивилизации.\", \"genre\": \"news\"},\n",
                "    {\"en\": \"The central bank raised interest rates to combat inflation.\", \"ru\": \"Центральный банк повысил процентные ставки для борьбы с инфляцией.\", \"genre\": \"news\"},\n",
                "    {\"en\": \"The new legislation will come into effect next month.\", \"ru\": \"Новое законодательство вступит в силу в следующем месяце.\", \"genre\": \"news\"},\n",
                "    {\"en\": \"The research team published groundbreaking findings in the medical journal.\", \"ru\": \"Исследовательская группа опубликовала революционные результаты в медицинском журнале.\", \"genre\": \"news\"},\n",
                "    {\"en\": \"The trade agreement between the two countries was signed yesterday.\", \"ru\": \"Торговое соглашение между двумя странами было подписано вчера.\", \"genre\": \"news\"},\n",
                "    {\"en\": \"The city council approved the construction of a new metro line.\", \"ru\": \"Городской совет одобрил строительство новой линии метро.\", \"genre\": \"news\"},\n",
                "    {\"en\": \"The investigation revealed serious violations of safety regulations.\", \"ru\": \"Расследование выявило серьезные нарушения правил безопасности.\", \"genre\": \"news\"},\n",
                "    {\"en\": \"The festival attracted thousands of visitors from around the world.\", \"ru\": \"Фестиваль привлек тысячи посетителей со всего мира.\", \"genre\": \"news\"},\n",
                "    {\"en\": \"The company announced plans to expand its operations to Asia.\", \"ru\": \"Компания объявила о планах расширить свою деятельность в Азии.\", \"genre\": \"news\"},\n",
                "    {\"en\": \"The environmental group organized a protest against deforestation.\", \"ru\": \"Экологическая группа организовала протест против вырубки лесов.\", \"genre\": \"news\"},\n",
                "    {\"en\": \"The new vaccine has shown promising results in clinical trials.\", \"ru\": \"Новая вакцина показала многообещающие результаты в клинических испытаниях.\", \"genre\": \"news\"},\n",
                "    {\"en\": \"The sports team won the championship for the third consecutive year.\", \"ru\": \"Спортивная команда выиграла чемпионат третий год подряд.\", \"genre\": \"news\"},\n",
                "    {\"en\": \"The government launched a new initiative to support small businesses.\", \"ru\": \"Правительство запустило новую инициативу по поддержке малого бизнеса.\", \"genre\": \"news\"},\n",
                "    {\"en\": \"The documentary film received critical acclaim at the international festival.\", \"ru\": \"Документальный фильм получил признание критиков на международном фестивале.\", \"genre\": \"news\"},\n",
                "    \n",
                "    # Fiction / Social (25 sentences)\n",
                "    {\"en\": \"She looked out the window, wondering if he would ever return.\", \"ru\": \"Она смотрела в окно, гадая, вернется ли он когда-нибудь.\", \"genre\": \"fiction\"},\n",
                "    {\"en\": \"The old house creaked in the wind, as if whispering secrets.\", \"ru\": \"Старый дом скрипел на ветру, словно нашептывая секреты.\", \"genre\": \"fiction\"},\n",
                "    {\"en\": \"'I can't believe you said that!' she exclaimed.\", \"ru\": \"— Не могу поверить, что ты это сказал! — воскликнула она.\", \"genre\": \"fiction\"},\n",
                "    {\"en\": \"He picked up the sword, feeling its weight in his hand.\", \"ru\": \"Он поднял меч, чувствуя его тяжесть в руке.\", \"genre\": \"fiction\"},\n",
                "    {\"en\": \"The stars shone brightly in the clear night sky.\", \"ru\": \"Звезды ярко сияли на чистом ночном небе.\", \"genre\": \"fiction\"},\n",
                "    {\"en\": \"She walked through the garden, her mind lost in memories of the past.\", \"ru\": \"Она шла по саду, ее мысли были погружены в воспоминания о прошлом.\", \"genre\": \"fiction\"},\n",
                "    {\"en\": \"The mysterious letter arrived on a rainy Tuesday morning.\", \"ru\": \"Таинственное письмо пришло дождливым вторничным утром.\", \"genre\": \"fiction\"},\n",
                "    {\"en\": \"'Why did you leave me?' he whispered into the darkness.\", \"ru\": \"— Почему ты оставил меня? — прошептал он в темноту.\", \"genre\": \"fiction\"},\n",
                "    {\"en\": \"The ancient book contained secrets that could change everything.\", \"ru\": \"Древняя книга содержала секреты, которые могли изменить все.\", \"genre\": \"fiction\"},\n",
                "    {\"en\": \"She felt a strange sensation, as if someone was watching her.\", \"ru\": \"Она почувствовала странное ощущение, словно кто-то наблюдает за ней.\", \"genre\": \"fiction\"},\n",
                "    {\"en\": \"The music filled the room, bringing tears to her eyes.\", \"ru\": \"Музыка наполнила комнату, вызывая слезы на ее глазах.\", \"genre\": \"fiction\"},\n",
                "    {\"en\": \"He had never seen such a beautiful sunset in all his years.\", \"ru\": \"Он никогда не видел такого красивого заката за все свои годы.\", \"genre\": \"fiction\"},\n",
                "    {\"en\": \"The old man smiled, knowing that his time had finally come.\", \"ru\": \"Старик улыбнулся, зная, что его время наконец пришло.\", \"genre\": \"fiction\"},\n",
                "    {\"en\": \"She opened the door slowly, afraid of what she might find inside.\", \"ru\": \"Она медленно открыла дверь, боясь того, что может найти внутри.\", \"genre\": \"fiction\"},\n",
                "    {\"en\": \"The forest seemed to come alive as the moon rose above the trees.\", \"ru\": \"Лес, казалось, оживал, когда луна поднималась над деревьями.\", \"genre\": \"fiction\"},\n",
                "    {\"en\": \"'Everything will be alright,' he said, though he didn't believe it himself.\", \"ru\": \"— Все будет хорошо, — сказал он, хотя сам в это не верил.\", \"genre\": \"fiction\"},\n",
                "    {\"en\": \"The photograph brought back memories she had tried so hard to forget.\", \"ru\": \"Фотография вернула воспоминания, которые она так старалась забыть.\", \"genre\": \"fiction\"},\n",
                "    {\"en\": \"He could hear the sound of footsteps approaching from behind.\", \"ru\": \"Он мог слышать звук шагов, приближающихся сзади.\", \"genre\": \"fiction\"},\n",
                "    {\"en\": \"The coffee tasted bitter, just like her mood that morning.\", \"ru\": \"Кофе был горьким, как и ее настроение в то утро.\", \"genre\": \"fiction\"},\n",
                "    {\"en\": \"She found herself standing at a crossroads, unsure which path to take.\", \"ru\": \"Она оказалась на перекрестке, не зная, какой путь выбрать.\", \"genre\": \"fiction\"},\n",
                "    {\"en\": \"The old photograph showed a family she had never known.\", \"ru\": \"Старая фотография показывала семью, которую она никогда не знала.\", \"genre\": \"fiction\"},\n",
                "    {\"en\": \"He whispered her name, and she turned around with a smile.\", \"ru\": \"Он прошептал ее имя, и она обернулась с улыбкой.\", \"genre\": \"fiction\"},\n",
                "    {\"en\": \"The storm raged outside, but inside the house, all was calm.\", \"ru\": \"Буря бушевала снаружи, но внутри дома все было спокойно.\", \"genre\": \"fiction\"},\n",
                "    {\"en\": \"She had waited her whole life for this moment, and now it was here.\", \"ru\": \"Она ждала этого момента всю свою жизнь, и теперь он наступил.\", \"genre\": \"fiction\"},\n",
                "    {\"en\": \"The last words he spoke would haunt her for the rest of her days.\", \"ru\": \"Последние слова, которые он произнес, будут преследовать ее до конца дней.\", \"genre\": \"fiction\"},\n",
                "]\n",
                "\n",
                "df_custom = pd.DataFrame(custom_dataset)\n",
                "print(f\"Custom dataset loaded: {len(df_custom)} sentences\")\n",
                "print(f\"News: {len(df_custom[df_custom['genre'] == 'news'])} sentences\")\n",
                "print(f\"Fiction: {len(df_custom[df_custom['genre'] == 'fiction'])} sentences\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "57b40ed8",
            "metadata": {},
            "source": [
                "## 5. Commercial System: Google Translate\n",
                "\n",
                "Using Google Translate via `deep-translator` for comparison with open-source models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ad73bfa5",
            "metadata": {},
            "outputs": [],
            "source": [
                "def translate_google(texts, src_lang, tgt_lang):\n",
                "    translator = GoogleTranslator(source=src_lang, target=tgt_lang)\n",
                "    translations = []\n",
                "    for text in tqdm(texts, desc=f\"Translating {src_lang}->{tgt_lang} (Google)\"):\n",
                "        try:\n",
                "            translations.append(translator.translate(text))\n",
                "        except Exception as e:\n",
                "            print(f\"Error translating: {e}\")\n",
                "            translations.append(\"\")\n",
                "    return translations"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "48b58ecf",
            "metadata": {},
            "source": [
                "## 6. Evaluation Metrics\n",
                "\n",
                "Using **BLEU** and **chrF++** scores via `sacrebleu` for standardized evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bb25338a",
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_metrics(predictions, references):\n",
                "    bleu = sacrebleu.corpus_bleu(predictions, [references])\n",
                "    chrf = sacrebleu.corpus_chrf(predictions, [references])\n",
                "    return {\"BLEU\": bleu.score, \"chrF++\": chrf.score}"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b68e1880",
            "metadata": {},
            "source": [
                "## 7. Evaluate on FLORES Dataset\n",
                "\n",
                "**Dataset:** [FLORES devtest subset](https://huggingface.co/datasets/openlanguagedata/flores_plus)\n",
                "\n",
                "FLORES is a challenging benchmark designed to test model robustness across diverse domains."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bba8a45e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load FLORES dataset for English-Russian\n",
                "dataset_flores = None\n",
                "\n",
                "try:\n",
                "    dataset_flores = load_dataset(\"facebook/flores\", \"eng-rus\", split=\"devtest\", trust_remote_code=True)\n",
                "    print(\"✓ Loaded FLORES from facebook/flores\")\n",
                "except Exception as e1:\n",
                "    try:\n",
                "        dataset_flores = load_dataset(\"facebook/flores\", \"eng-rus\", split=\"devtest\")\n",
                "        print(\"✓ Loaded FLORES from facebook/flores\")\n",
                "    except Exception as e2:\n",
                "        try:\n",
                "            load_kwargs = {\"trust_remote_code\": True}\n",
                "            if hf_token:\n",
                "                load_kwargs[\"token\"] = hf_token\n",
                "            dataset_flores = load_dataset(\"openlanguagedata/flores_plus\", \"default\", split=\"devtest\", **load_kwargs)\n",
                "            print(\"✓ Loaded FLORES with default config\")\n",
                "        except Exception as e3:\n",
                "            print(f\"\\n⚠ Could not load FLORES dataset. Error: {e3}\")\n",
                "            print(\"\\nContinuing with custom dataset only (this is sufficient for the task).\")\n",
                "            dataset_flores = None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "04f961f7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract English and Russian sentences from FLORES\n",
                "if dataset_flores is not None:\n",
                "    cols = list(dataset_flores.column_names)\n",
                "    print(f\"FLORES columns available: {cols[:10]}...\")\n",
                "    \n",
                "    # Check if this is the new FLORES structure with language metadata\n",
                "    if 'text' in cols and ('iso_639_3' in cols or 'iso_15924' in cols):\n",
                "        print(\"Detected new FLORES structure with language metadata\")\n",
                "        \n",
                "        try:\n",
                "            df_flores = dataset_flores.to_pandas()\n",
                "            \n",
                "            # Filter English and Russian sentences\n",
                "            if 'iso_639_3' in cols and 'iso_15924' in cols:\n",
                "                eng_mask = (df_flores['iso_639_3'] == 'eng') & (df_flores['iso_15924'] == 'Latn')\n",
                "                rus_mask = (df_flores['iso_639_3'] == 'rus') & (df_flores['iso_15924'] == 'Cyrl')\n",
                "            elif 'iso_639_3' in cols:\n",
                "                eng_mask = df_flores['iso_639_3'] == 'eng'\n",
                "                rus_mask = df_flores['iso_639_3'] == 'rus'\n",
                "            else:\n",
                "                raise ValueError(\"Cannot determine language from available columns\")\n",
                "            \n",
                "            flores_en_df = df_flores[eng_mask].sort_values('id') if 'id' in cols else df_flores[eng_mask]\n",
                "            flores_ru_df = df_flores[rus_mask].sort_values('id') if 'id' in cols else df_flores[rus_mask]\n",
                "            \n",
                "            # Align by id to ensure parallel sentences\n",
                "            if 'id' in cols:\n",
                "                merged = pd.merge(flores_en_df, flores_ru_df, on='id', suffixes=('_en', '_ru'))\n",
                "                flores_en = merged['text_en'].tolist()\n",
                "                flores_ru = merged['text_ru'].tolist()\n",
                "            else:\n",
                "                min_len = min(len(flores_en_df), len(flores_ru_df))\n",
                "                flores_en = flores_en_df['text'].head(min_len).tolist()\n",
                "                flores_ru = flores_ru_df['text'].head(min_len).tolist()\n",
                "            \n",
                "            print(f\"✓ Extracted {len(flores_en)} parallel English-Russian sentence pairs\")\n",
                "        except Exception as e:\n",
                "            print(f\"⚠ Error processing FLORES structure: {e}\")\n",
                "            dataset_flores = None\n",
                "    \n",
                "    # Fallback: Check for direct language columns (old structure)\n",
                "    flores_extracted = False\n",
                "    if dataset_flores is not None:\n",
                "        try:\n",
                "            flores_en\n",
                "            flores_extracted = True\n",
                "        except NameError:\n",
                "            flores_extracted = False\n",
                "    \n",
                "    if dataset_flores is not None and not flores_extracted:\n",
                "        if 'eng_Latn' in cols and 'rus_Cyrl' in cols:\n",
                "            flores_en = dataset_flores['eng_Latn']\n",
                "            flores_ru = dataset_flores['rus_Cyrl']\n",
                "            print(\"✓ Using eng_Latn and rus_Cyrl columns\")\n",
                "        else:\n",
                "            print(f\"⚠ Could not extract English/Russian sentences. Available columns: {cols[:20]}\")\n",
                "            dataset_flores = None\n",
                "    \n",
                "    # Limit to first 100 sentences for faster evaluation\n",
                "    if dataset_flores is not None:\n",
                "        try:\n",
                "            _ = flores_en\n",
                "            _ = flores_ru\n",
                "            eval_size = min(100, len(flores_en))\n",
                "            flores_en_eval = flores_en[:eval_size]\n",
                "            flores_ru_eval = flores_ru[:eval_size]\n",
                "            print(f\"Evaluating on {eval_size} FLORES sentences\")\n",
                "        except NameError:\n",
                "            print(\"FLORES dataset not loaded or extraction failed. Skipping FLORES evaluation.\")\n",
                "            dataset_flores = None\n",
                "else:\n",
                "    print(\"FLORES dataset not loaded. Skipping FLORES evaluation.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3827a630",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate NLLB on FLORES\n",
                "if dataset_flores is not None:\n",
                "    print(\"\\n=== Evaluating NLLB on FLORES ===\")\n",
                "    \n",
                "    print(\"Translating En->Ru...\")\n",
                "    nllb_en_ru_flores = translate_nllb(flores_en_eval, 'en', 'ru', model_nllb, tokenizer_nllb)\n",
                "    metrics_nllb_en_ru_flores = compute_metrics(nllb_en_ru_flores, flores_ru_eval)\n",
                "    print(f\"NLLB En->Ru: BLEU={metrics_nllb_en_ru_flores['BLEU']:.2f}, chrF++={metrics_nllb_en_ru_flores['chrF++']:.2f}\")\n",
                "    \n",
                "    print(\"Translating Ru->En...\")\n",
                "    nllb_ru_en_flores = translate_nllb(flores_ru_eval, 'ru', 'en', model_nllb, tokenizer_nllb)\n",
                "    metrics_nllb_ru_en_flores = compute_metrics(nllb_ru_en_flores, flores_en_eval)\n",
                "    print(f\"NLLB Ru->En: BLEU={metrics_nllb_ru_en_flores['BLEU']:.2f}, chrF++={metrics_nllb_ru_en_flores['chrF++']:.2f}\")\n",
                "    \n",
                "    # Evaluate Helsinki on FLORES\n",
                "    print(\"\\n=== Evaluating Helsinki on FLORES ===\")\n",
                "    \n",
                "    print(\"Translating En->Ru...\")\n",
                "    helsinki_en_ru_flores = translate_helsinki(flores_en_eval, 'en', 'ru')\n",
                "    metrics_helsinki_en_ru_flores = compute_metrics(helsinki_en_ru_flores, flores_ru_eval)\n",
                "    print(f\"Helsinki En->Ru: BLEU={metrics_helsinki_en_ru_flores['BLEU']:.2f}, chrF++={metrics_helsinki_en_ru_flores['chrF++']:.2f}\")\n",
                "    \n",
                "    print(\"Translating Ru->En...\")\n",
                "    helsinki_ru_en_flores = translate_helsinki(flores_ru_eval, 'ru', 'en')\n",
                "    metrics_helsinki_ru_en_flores = compute_metrics(helsinki_ru_en_flores, flores_en_eval)\n",
                "    print(f\"Helsinki Ru->En: BLEU={metrics_helsinki_ru_en_flores['BLEU']:.2f}, chrF++={metrics_helsinki_ru_en_flores['chrF++']:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "00a6371e",
            "metadata": {},
            "source": [
                "## 8. Evaluate on Custom Dataset\n",
                "\n",
                "Evaluating all models on our custom 50-sentence dataset (both directions)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a8b4fae7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract English and Russian sentences from custom dataset\n",
                "custom_en = df_custom['en'].tolist()\n",
                "custom_ru = df_custom['ru'].tolist()\n",
                "\n",
                "print(f\"Evaluating on {len(custom_en)} custom sentences\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8f540138",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate NLLB on custom dataset\n",
                "print(\"\\n=== Evaluating NLLB on Custom Dataset ===\")\n",
                "\n",
                "print(\"Translating En->Ru...\")\n",
                "nllb_en_ru_custom = translate_nllb(custom_en, 'en', 'ru', model_nllb, tokenizer_nllb)\n",
                "metrics_nllb_en_ru_custom = compute_metrics(nllb_en_ru_custom, custom_ru)\n",
                "print(f\"NLLB En->Ru: BLEU={metrics_nllb_en_ru_custom['BLEU']:.2f}, chrF++={metrics_nllb_en_ru_custom['chrF++']:.2f}\")\n",
                "\n",
                "print(\"Translating Ru->En...\")\n",
                "nllb_ru_en_custom = translate_nllb(custom_ru, 'ru', 'en', model_nllb, tokenizer_nllb)\n",
                "metrics_nllb_ru_en_custom = compute_metrics(nllb_ru_en_custom, custom_en)\n",
                "print(f\"NLLB Ru->En: BLEU={metrics_nllb_ru_en_custom['BLEU']:.2f}, chrF++={metrics_nllb_ru_en_custom['chrF++']:.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "02642772",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate Helsinki on custom dataset\n",
                "print(\"\\n=== Evaluating Helsinki on Custom Dataset ===\")\n",
                "\n",
                "print(\"Translating En->Ru...\")\n",
                "helsinki_en_ru_custom = translate_helsinki(custom_en, 'en', 'ru')\n",
                "metrics_helsinki_en_ru_custom = compute_metrics(helsinki_en_ru_custom, custom_ru)\n",
                "print(f\"Helsinki En->Ru: BLEU={metrics_helsinki_en_ru_custom['BLEU']:.2f}, chrF++={metrics_helsinki_en_ru_custom['chrF++']:.2f}\")\n",
                "\n",
                "print(\"Translating Ru->En...\")\n",
                "helsinki_ru_en_custom = translate_helsinki(custom_ru, 'ru', 'en')\n",
                "metrics_helsinki_ru_en_custom = compute_metrics(helsinki_ru_en_custom, custom_en)\n",
                "print(f\"Helsinki Ru->En: BLEU={metrics_helsinki_ru_en_custom['BLEU']:.2f}, chrF++={metrics_helsinki_ru_en_custom['chrF++']:.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1d218a3d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate Google Translate on custom dataset\n",
                "print(\"\\n=== Evaluating Google Translate on Custom Dataset ===\")\n",
                "\n",
                "print(\"Translating En->Ru...\")\n",
                "google_en_ru_custom = translate_google(custom_en, 'en', 'ru')\n",
                "metrics_google_en_ru_custom = compute_metrics(google_en_ru_custom, custom_ru)\n",
                "print(f\"Google En->Ru: BLEU={metrics_google_en_ru_custom['BLEU']:.2f}, chrF++={metrics_google_en_ru_custom['chrF++']:.2f}\")\n",
                "\n",
                "print(\"Translating Ru->En...\")\n",
                "google_ru_en_custom = translate_google(custom_ru, 'ru', 'en')\n",
                "metrics_google_ru_en_custom = compute_metrics(google_ru_en_custom, custom_en)\n",
                "print(f\"Google Ru->En: BLEU={metrics_google_ru_en_custom['BLEU']:.2f}, chrF++={metrics_google_ru_en_custom['chrF++']:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5d47649f",
            "metadata": {},
            "source": [
                "## 9. Results Summary and Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e1b80c1f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create results table for custom dataset\n",
                "results_custom = pd.DataFrame({\n",
                "    'Model': ['NLLB-200', 'NLLB-200', 'Helsinki-NLP', 'Helsinki-NLP', 'Google Translate', 'Google Translate'],\n",
                "    'Direction': ['En→Ru', 'Ru→En', 'En→Ru', 'Ru→En', 'En→Ru', 'Ru→En'],\n",
                "    'BLEU': [\n",
                "        metrics_nllb_en_ru_custom['BLEU'],\n",
                "        metrics_nllb_ru_en_custom['BLEU'],\n",
                "        metrics_helsinki_en_ru_custom['BLEU'],\n",
                "        metrics_helsinki_ru_en_custom['BLEU'],\n",
                "        metrics_google_en_ru_custom['BLEU'],\n",
                "        metrics_google_ru_en_custom['BLEU']\n",
                "    ],\n",
                "    'chrF++': [\n",
                "        metrics_nllb_en_ru_custom['chrF++'],\n",
                "        metrics_nllb_ru_en_custom['chrF++'],\n",
                "        metrics_helsinki_en_ru_custom['chrF++'],\n",
                "        metrics_helsinki_ru_en_custom['chrF++'],\n",
                "        metrics_google_en_ru_custom['chrF++'],\n",
                "        metrics_google_ru_en_custom['chrF++']\n",
                "    ]\n",
                "})\n",
                "\n",
                "print(\"=== Results on Custom Dataset ===\")\n",
                "print(results_custom.to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dd5c7b26",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate by genre\n",
                "print(\"\\n=== Results by Genre ===\")\n",
                "\n",
                "# News genre\n",
                "news_mask = df_custom['genre'] == 'news'\n",
                "news_en = df_custom[news_mask]['en'].tolist()\n",
                "news_ru = df_custom[news_mask]['ru'].tolist()\n",
                "\n",
                "# Fiction genre\n",
                "fiction_mask = df_custom['genre'] == 'fiction'\n",
                "fiction_en = df_custom[fiction_mask]['en'].tolist()\n",
                "fiction_ru = df_custom[fiction_mask]['ru'].tolist()\n",
                "\n",
                "# Translate news\n",
                "nllb_news_en_ru = translate_nllb(news_en, 'en', 'ru', model_nllb, tokenizer_nllb)\n",
                "helsinki_news_en_ru = translate_helsinki(news_en, 'en', 'ru')\n",
                "google_news_en_ru = translate_google(news_en, 'en', 'ru')\n",
                "\n",
                "# Translate fiction\n",
                "nllb_fiction_en_ru = translate_nllb(fiction_en, 'en', 'ru', model_nllb, tokenizer_nllb)\n",
                "helsinki_fiction_en_ru = translate_helsinki(fiction_en, 'en', 'ru')\n",
                "google_fiction_en_ru = translate_google(fiction_en, 'en', 'ru')\n",
                "\n",
                "# Compute metrics by genre\n",
                "results_by_genre = pd.DataFrame({\n",
                "    'Model': ['NLLB-200', 'Helsinki-NLP', 'Google Translate', 'NLLB-200', 'Helsinki-NLP', 'Google Translate'],\n",
                "    'Genre': ['News', 'News', 'News', 'Fiction', 'Fiction', 'Fiction'],\n",
                "    'BLEU': [\n",
                "        compute_metrics(nllb_news_en_ru, news_ru)['BLEU'],\n",
                "        compute_metrics(helsinki_news_en_ru, news_ru)['BLEU'],\n",
                "        compute_metrics(google_news_en_ru, news_ru)['BLEU'],\n",
                "        compute_metrics(nllb_fiction_en_ru, fiction_ru)['BLEU'],\n",
                "        compute_metrics(helsinki_fiction_en_ru, fiction_ru)['BLEU'],\n",
                "        compute_metrics(google_fiction_en_ru, fiction_ru)['BLEU']\n",
                "    ],\n",
                "    'chrF++': [\n",
                "        compute_metrics(nllb_news_en_ru, news_ru)['chrF++'],\n",
                "        compute_metrics(helsinki_news_en_ru, news_ru)['chrF++'],\n",
                "        compute_metrics(google_news_en_ru, news_ru)['chrF++'],\n",
                "        compute_metrics(nllb_fiction_en_ru, fiction_ru)['chrF++'],\n",
                "        compute_metrics(helsinki_fiction_en_ru, fiction_ru)['chrF++'],\n",
                "        compute_metrics(google_fiction_en_ru, fiction_ru)['chrF++']\n",
                "    ]\n",
                "})\n",
                "\n",
                "print(results_by_genre.to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ab5b6763",
            "metadata": {},
            "source": [
                "## 10. Example Translation Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "61d9aae4",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select a few examples for detailed analysis\n",
                "example_indices = [0, 5, 10, 20, 30, 40]  # Mix of news and fiction\n",
                "\n",
                "print(\"=== Example Translations ===\")\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "\n",
                "for idx in example_indices:\n",
                "    if idx < len(df_custom):\n",
                "        row = df_custom.iloc[idx]\n",
                "        print(f\"\\nExample {idx+1} ({row['genre']}):\")\n",
                "        print(f\"Source (EN): {row['en']}\")\n",
                "        print(f\"Reference (RU): {row['ru']}\")\n",
                "        print(f\"NLLB: {nllb_en_ru_custom[idx]}\")\n",
                "        print(f\"Helsinki: {helsinki_en_ru_custom[idx]}\")\n",
                "        print(f\"Google: {google_en_ru_custom[idx]}\")\n",
                "        print(\"-\"*80)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3eedd7bb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find best and worst examples\n",
                "def simple_similarity(pred, ref):\n",
                "    pred_words = set(pred.lower().split())\n",
                "    ref_words = set(ref.lower().split())\n",
                "    if len(ref_words) == 0:\n",
                "        return 0\n",
                "    return len(pred_words & ref_words) / len(ref_words)\n",
                "\n",
                "# Calculate similarities for NLLB\n",
                "nllb_similarities = [simple_similarity(nllb_en_ru_custom[i], custom_ru[i]) for i in range(len(custom_ru))]\n",
                "best_nllb_idx = nllb_similarities.index(max(nllb_similarities))\n",
                "worst_nllb_idx = nllb_similarities.index(min(nllb_similarities))\n",
                "\n",
                "print(\"\\n=== Best NLLB Translation ===\")\n",
                "print(f\"Source (EN): {custom_en[best_nllb_idx]}\")\n",
                "print(f\"Reference (RU): {custom_ru[best_nllb_idx]}\")\n",
                "print(f\"NLLB: {nllb_en_ru_custom[best_nllb_idx]}\")\n",
                "print(f\"Genre: {df_custom.iloc[best_nllb_idx]['genre']}\")\n",
                "\n",
                "print(\"\\n=== Worst NLLB Translation ===\")\n",
                "print(f\"Source (EN): {custom_en[worst_nllb_idx]}\")\n",
                "print(f\"Reference (RU): {custom_ru[worst_nllb_idx]}\")\n",
                "print(f\"NLLB: {nllb_en_ru_custom[worst_nllb_idx]}\")\n",
                "print(f\"Genre: {df_custom.iloc[worst_nllb_idx]['genre']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "08cc3e98",
            "metadata": {},
            "source": [
                "## 11. Final Results Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a339d1d7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create comprehensive results summary\n",
                "print(\"=\"*80)\n",
                "print(\"COMPREHENSIVE RESULTS SUMMARY\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Custom dataset results\n",
                "print(\"\\n### Custom Dataset Results (50 sentences: 25 news + 25 fiction)\")\n",
                "print(results_custom.to_string(index=False))\n",
                "\n",
                "# FLORES results (if available)\n",
                "if dataset_flores is not None:\n",
                "    print(\"\\n### FLORES Dataset Results (devtest subset)\")\n",
                "    results_flores = pd.DataFrame({\n",
                "        'Model': ['NLLB-200', 'NLLB-200', 'Helsinki-NLP', 'Helsinki-NLP'],\n",
                "        'Direction': ['En→Ru', 'Ru→En', 'En→Ru', 'Ru→En'],\n",
                "        'BLEU': [\n",
                "            metrics_nllb_en_ru_flores['BLEU'],\n",
                "            metrics_nllb_ru_en_flores['BLEU'],\n",
                "            metrics_helsinki_en_ru_flores['BLEU'],\n",
                "            metrics_helsinki_ru_en_flores['BLEU']\n",
                "        ],\n",
                "        'chrF++': [\n",
                "            metrics_nllb_en_ru_flores['chrF++'],\n",
                "            metrics_nllb_ru_en_flores['chrF++'],\n",
                "            metrics_helsinki_en_ru_flores['chrF++'],\n",
                "            metrics_helsinki_ru_en_flores['chrF++']\n",
                "        ]\n",
                "    })\n",
                "    print(results_flores.to_string(index=False))\n",
                "\n",
                "# Genre-specific results\n",
                "print(\"\\n### Results by Genre (Custom Dataset)\")\n",
                "print(results_by_genre.to_string(index=False))\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"Evaluation Complete!\")\n",
                "print(\"=\"*80)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
